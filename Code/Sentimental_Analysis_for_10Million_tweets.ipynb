{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /Users/shripalshah/opt/anaconda3/lib/python3.9/site-packages (4.9.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /Users/shripalshah/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /Users/shripalshah/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /Users/shripalshah/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shripalshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/shripalshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shripalshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shripalshah/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lPT2wQtaLFj8"
   },
   "source": [
    "#Fetching tweets using Twitter API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkNwWb4-Ak5m",
    "outputId": "0d4519f1-d5d1-4ac7-c89e-c4827b9073ee"
   },
   "outputs": [],
   "source": [
    "# Python Script to Extract tweets of a\n",
    "# particular Hashtag using Tweepy and Pandas\n",
    "\n",
    "# import modules\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "\n",
    "# function to display data of each tweet\n",
    "def printtweetdata(n, ith_tweet):\n",
    "\t\tprint()\n",
    "\t\tprint(f\"Tweet {n}:\")\n",
    "\t\tprint(f\"Username:{ith_tweet[0]}\")\n",
    "\t\tprint(f\"Description:{ith_tweet[1]}\")\n",
    "\t\tprint(f\"Location:{ith_tweet[2]}\")\n",
    "\t\tprint(f\"Following Count:{ith_tweet[3]}\")\n",
    "\t\tprint(f\"Follower Count:{ith_tweet[4]}\")\n",
    "\t\tprint(f\"Total Tweets:{ith_tweet[5]}\")\n",
    "\t\tprint(f\"Retweet Count:{ith_tweet[6]}\")\n",
    "\t\tprint(f\"Tweet Text:{ith_tweet[7]}\")\n",
    "\t\tprint(f\"Hashtags Used:{ith_tweet[8]}\")\n",
    "\t\tprint(f\"Languages Used:{ith_tweet[9]}\")\n",
    "\n",
    "\n",
    "# function to perform data extraction\n",
    "def scrape(words, date_since, numtweet):\n",
    "\n",
    "\t\t# Creating DataFrame using pandas\n",
    "\t\tdb = pd.DataFrame(columns=['username',\n",
    "\t\t\t\t\t\t\t\t'description',\n",
    "\t\t\t\t\t\t\t\t'location',\n",
    "\t\t\t\t\t\t\t\t'following',\n",
    "\t\t\t\t\t\t\t\t'followers',\n",
    "\t\t\t\t\t\t\t\t'totaltweets',\n",
    "\t\t\t\t\t\t\t\t'retweetcount',\n",
    "\t\t\t\t\t\t\t\t'text',\n",
    "\t\t\t\t\t\t\t\t'hashtags',\n",
    "\t\t\t\t\t\t\t\t'lang'])\n",
    "\n",
    "\t\t# We are using .Cursor() to search\n",
    "\t\t# through twitter for the required tweets.\n",
    "\t\t# The number of tweets can be\n",
    "\t\t# restricted using .items(number of tweets)\n",
    "\t\ttweets = tweepy.Cursor(api.search,\n",
    "\t\t\t\t\t\t\twords,\n",
    "\t\t\t\t\t\t\tsince_id=date_since,\n",
    "\t\t\t\t\t\t\ttweet_mode='extended').items(numtweet)\n",
    "\n",
    "\n",
    "\t\t# .Cursor() returns an iterable object. Each item in\n",
    "\t\t# the iterator has various attributes\n",
    "\t\t# that you can access to\n",
    "\t\t# get information about each tweet\n",
    "\t\tlist_tweets = [tweet for tweet in tweets]\n",
    "\n",
    "\t\t# Counter to maintain Tweet Count\n",
    "\t\ti = 1\n",
    "\n",
    "\t\t# we will iterate over each tweet in the\n",
    "\t\t# list for extracting information about each tweet\n",
    "\t\tfor tweet in list_tweets:\n",
    "\t\t\t\tusername = tweet.user.screen_name\n",
    "\t\t\t\tdescription = tweet.user.description\n",
    "\t\t\t\tlocation = tweet.user.location\n",
    "\t\t\t\tfollowing = tweet.user.friends_count\n",
    "\t\t\t\tfollowers = tweet.user.followers_count\n",
    "\t\t\t\ttotaltweets = tweet.user.statuses_count\n",
    "\t\t\t\tretweetcount = tweet.retweet_count\n",
    "\t\t\t\thashtags = tweet.entities['hashtags']\n",
    "\t\t\t\tlanguage = tweet.lang\n",
    "\n",
    "\t\t\t\t# Retweets can be distinguished by\n",
    "\t\t\t\t# a retweeted_status attribute,\n",
    "\t\t\t\t# in case it is an invalid reference,\n",
    "\t\t\t\t# except block will be executed\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\t\ttext = tweet.retweeted_status.full_text\n",
    "\t\t\t\texcept AttributeError:\n",
    "\t\t\t\t\t\ttext = tweet.full_text\n",
    "\t\t\t\thashtext = list()\n",
    "\t\t\t\tfor j in range(0, len(hashtags)):\n",
    "\t\t\t\t\t\thashtext.append(hashtags[j]['text'])\n",
    "\n",
    "\t\t\t\t# Here we are appending all the\n",
    "\t\t\t\t# extracted information in the DataFrame\n",
    "\t\t\t\tith_tweet = [username, description,\n",
    "\t\t\t\t\t\t\tlocation, following,\n",
    "\t\t\t\t\t\t\tfollowers, totaltweets,\n",
    "\t\t\t\t\t\t\tretweetcount, text, hashtext, language]\n",
    "\t\t\t\tdb.loc[len(db)] = ith_tweet\n",
    "                \n",
    "\n",
    "\t\t\t\t# Function call to print tweet data on screen\n",
    "\t\t\t\tprinttweetdata(i, ith_tweet)\n",
    "\t\t\t\ti = i+1\n",
    "\t\tfilename = 'scraped_tweets.csv'\n",
    "\n",
    "\t\t# we will save our database as a CSV file.\n",
    "\t\tdb.to_csv(filename)\n",
    "        \n",
    "#return scrape[words, date_since, numtweet].head(10)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\t\t# Enter your own credentials obtained\n",
    "\t\t# from your developer account\n",
    "\t\tconsumer_key = \" \"\n",
    "\t\tconsumer_secret = \" \"\n",
    "\t\taccess_key = \" \"\n",
    "\t\taccess_secret = \" \"\n",
    "\n",
    "\n",
    "\t\tauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\t\tauth.set_access_token(access_key, access_secret)\n",
    "\t\tapi = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "\t\t# Enter Hashtag and initial date\n",
    "\t\tprint(\"Enter Twitter HashTag to search for\")\n",
    "\t\twords = \"UkraineRussiaWar\"\n",
    "\t\tprint(\"Enter Date since The Tweets are required in yyyy-mm--dd\")\n",
    "\t\tdate_since = \"2022-03-26\"\n",
    "\t\t# number of tweets you want to extract in one run\n",
    "\t\tnumtweet = 10000000\n",
    "\t\tscrape(words, date_since, numtweet)\n",
    "\t\tprint('Scraping has completed!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ui4HTlf5iO5S"
   },
   "source": [
    "#Importing Libraries, Modules, Utilities, Plotting, NLTK, Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6y5Nn8lvQcd7"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "# utilities\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oGvjzpGQfMB"
   },
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zH5uidlzjOS1"
   },
   "source": [
    "#Starting Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyWTintiQjnX"
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.master(\"local[*]\")\n",
    "                     .config(\"spark.driver.memory\", '1g')\n",
    "          .config('spark.network.timeout','3601s')\n",
    "          .config('spark.executor.heartbeatInterval','3600s')\n",
    "                     .appName(\"Sentiment Analysis\")\n",
    "                     .getOrCreate())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WN0nTmvKjSOf"
   },
   "source": [
    "#Reading Fetched Tweets File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xeei34q3QlCX"
   },
   "outputs": [],
   "source": [
    "df  = spark.read.format(\"csv\").option(\"delimiter\",\",\").option(\"multiline\",True).option(\"header\",True).option(\"inferSchema\",True).option('ignoreTrailingWhiteSpace',True).option(\"unescapedQuoteHandling\",\"STOP_AT_CLOSING_QUOTE\").load(\"/content/scraped_tweets.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yafuBeIujXP8"
   },
   "source": [
    "#Data Cleaning/ Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyV-vVy0QmWT"
   },
   "outputs": [],
   "source": [
    "df_en = df.filter(df.lang == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHZMX2CCQngO"
   },
   "outputs": [],
   "source": [
    "df_en.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mia4v1RIQoYG"
   },
   "outputs": [],
   "source": [
    "df2 = df_en.drop(\"description\",\"following\",\"followers\",\"totaltweets\", \"retweetcount\", \"hashtags\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7mpVfQIGjce8"
   },
   "source": [
    "#Data Preprocessing\n",
    " ## Removing Links\n",
    " ## Removing Numeric Values\n",
    " ## Removing Emoticons\n",
    " ## Removing Symbols\n",
    " ## Removing Blank Spaces\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYe6DEKZQpZA"
   },
   "outputs": [],
   "source": [
    "def preprocessing(sparkDF,col):\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'http\\S+', ''))\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'@\\w+', ''))\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'#\\w+', ''))\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'RT', ''))\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r':', ''))\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'[^A-Za-z0-9]+', ' '))\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'[0-9]+', ''))\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'\\-', ''))\n",
    "    sparkDF = sparkDF.withColumn(col, F.regexp_replace(col, r'[ ]+', ' '))\n",
    "    sparkDF = sparkDF.withColumn(col, F.trim(sparkDF[col]))\n",
    "\n",
    "    return sparkDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RymWngDWQqaS"
   },
   "outputs": [],
   "source": [
    "df2 = preprocessing(df2,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4_nG8RlQrPG"
   },
   "outputs": [],
   "source": [
    "#preprocessed Tweets\n",
    "df2.show(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zCsnsD0akHms"
   },
   "source": [
    "#Fetch column: “text” because we need only that column for extracting sentiments from users and for that we need to convert our data frame into RDD(best suited for processing unstructured data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Al-OBpG9QsVh"
   },
   "outputs": [],
   "source": [
    "Tweets_rdd = df2.select(\"text\").rdd.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6yQh12fQtg3"
   },
   "outputs": [],
   "source": [
    "header = Tweets_rdd.first()\n",
    "data_rmv_col = Tweets_rdd.filter(lambda row: row != header)\n",
    "\n",
    "lowerCase_sentRDD = data_rmv_col.map(lambda x : x.lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CWOn3-oJkW0W"
   },
   "source": [
    "#Now split each sentence into words, also called word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWXPJLoQQuzy"
   },
   "outputs": [],
   "source": [
    "def sent_TokenizeFunct(x):\n",
    "    return nltk.sent_tokenize(x)\n",
    "sentenceTokenizeRDD = lowerCase_sentRDD.map(sent_TokenizeFunct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3jIs5FSYkYh5"
   },
   "source": [
    "#Now split each sentence into words, also called word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0SjmUDNQ791"
   },
   "outputs": [],
   "source": [
    "def word_TokenizeFunct(x):\n",
    "    splitted = [word for line in x for word in line.split()]\n",
    "    return splitted\n",
    "wordTokenizeRDD = sentenceTokenizeRDD.map(word_TokenizeFunct)\n",
    "wordTokenizeRDD.take(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "L_V2aSRYkdua"
   },
   "source": [
    "#To move ahead first we will clean our data, here we’re gonna remove stopwords, punctuations, and empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lcx_fORvQwDP"
   },
   "outputs": [],
   "source": [
    "def removeStopWordsFunct(x):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    filteredSentence = [w for w in x if not w in stop_words]\n",
    "    return filteredSentence\n",
    "stopwordRDD = wordTokenizeRDD.map(removeStopWordsFunct)\n",
    "def removePunctuationsFunct(x):\n",
    "    list_punct=list(string.punctuation)\n",
    "    filtered = [''.join(c for c in s if c not in list_punct) for s in x] \n",
    "    filtered_space = [s for s in filtered if s] #remove empty space \n",
    "    return filtered\n",
    "rmvPunctRDD = stopwordRDD.map(removePunctuationsFunct)\n",
    "\n",
    "rmvPunctRDD.take(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IMzf9FIJkhFi"
   },
   "source": [
    "#Stemming and Lemmatization are the basic text processing methods for English text. The goal of both of them is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. I have skipped Stemming because it is not an efficient method as sometimes it produces words that are not even close to the actual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFpM5Q4mQ1LW"
   },
   "outputs": [],
   "source": [
    "def lemmatizationFunct(x):\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    finalLem = [lemmatizer.lemmatize(s) for s in x]\n",
    "    return finalLem\n",
    "lem_wordsRDD = rmvPunctRDD.map(lemmatizationFunct)\n",
    "\n",
    "lem_wordsRDD.take(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "W91Wk7sPkmE2"
   },
   "source": [
    "#Our next task is a little tricky, we have to extract keyphrases(also called Noun phrases). So first we need to join “lem_wordsRDD” tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ydeZYZtQ3iP"
   },
   "outputs": [],
   "source": [
    "def joinTokensFunct(x):\n",
    "    joinedTokens_list = []\n",
    "    x = \" \".join(x)\n",
    "    return [x]\n",
    "joinedTokens = lem_wordsRDD.map(joinTokensFunct)\n",
    "joinedTokens.take(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wRxTQ87fkoO7"
   },
   "source": [
    "#From the above step we roughly got all the key phrases the users are talking about. Now categorize these key phrases into Positive, Negative, or Neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8Oi_9XzQ5VY"
   },
   "outputs": [],
   "source": [
    "def sentimentWordsFunct(x):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer() \n",
    "    senti_list_temp = [] \n",
    "    for i in x:\n",
    "        y = ''.join(i) \n",
    "        vs = analyzer.polarity_scores(y)\n",
    "        senti_list_temp.append((y, vs))\n",
    "        senti_list_temp = [w for w in senti_list_temp if w]    \n",
    "    sentiment_list  = []\n",
    "    for j in senti_list_temp:\n",
    "        first = j[0]\n",
    "        second = j[1]\n",
    "    \n",
    "        for (k,v) in second.items():\n",
    "            if k == 'compound':\n",
    "                if v < 0.0:\n",
    "                    sentiment_list.append((first, \"Negative\"))\n",
    "                elif v == 0.0:\n",
    "                    sentiment_list.append((first, \"Neutral\"))\n",
    "                else:\n",
    "                    sentiment_list.append((first, \"Positive\"))\n",
    "    return sentiment_list\n",
    "\n",
    "sentimentRDD = joinedTokens.flatMap(sentimentWordsFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfGl-tEeRAZs"
   },
   "outputs": [],
   "source": [
    "sentimentRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDldNgwMRC99"
   },
   "outputs": [],
   "source": [
    "data = sentimentRDD.toDF()\n",
    "data.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_o0_KEGRkvV7"
   },
   "source": [
    "#Sentiment Analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvYB9rdpREh4"
   },
   "outputs": [],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGiarU-gRFrx"
   },
   "outputs": [],
   "source": [
    "print((data.count(), len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xAajT-QRGz1"
   },
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"sentiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kECWuProRIQa"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select _2 as Sentiments, count(_2) as Counts from sentiments group by _2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wjhu9ILFRKDJ"
   },
   "outputs": [],
   "source": [
    "data.toPandas().to_csv(\"tweet_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bu9RZwfURLEM"
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"/content/tweet_df.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tsm9anPLk0oZ"
   },
   "source": [
    "#Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11nbtPXhRMxz"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfJ8GIfmROZZ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "sns.countplot(x=\"_2\", data=tweets_df, palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zs422-SVRPee"
   },
   "outputs": [],
   "source": [
    "tweet_neg = tweets_df.loc[tweets_df['_2'] == 'Negative'].reset_index(drop=True)\n",
    "tweet_net = tweets_df.loc[tweets_df['_2'] == 'Neutral'].reset_index(drop=True)\n",
    "tweet_pos = tweets_df.loc[tweets_df['_2'] == 'Positive'].reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eHpXO_55k8l_"
   },
   "source": [
    "#EDA Visualization for Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqE8HP-KRQyA"
   },
   "outputs": [],
   "source": [
    "stopwords_set = set(STOPWORDS)\n",
    "wordcloud = WordCloud(background_color='black',\n",
    "                     stopwords = stopwords_set,\n",
    "                      max_words = 300,\n",
    "                      max_font_size = 40,\n",
    "                      scale = 2,\n",
    "                      random_state=42\n",
    "                     ).generate(str(tweet_neg['_1']))\n",
    "\n",
    "print(wordcloud)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yTYQXqCOlFk0"
   },
   "source": [
    "#EDA Visualization for Neutral Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RY2LW_LeRSCz"
   },
   "outputs": [],
   "source": [
    "stopwords_set = set(STOPWORDS)\n",
    "wordcloud = WordCloud(background_color='black',\n",
    "                     stopwords = stopwords_set,\n",
    "                      max_words = 300,\n",
    "                      max_font_size = 40,\n",
    "                      scale = 2,\n",
    "                      random_state=42\n",
    "                     ).generate(str(tweet_net['_1']))\n",
    "\n",
    "print(wordcloud)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi4XjAc0lLmS"
   },
   "source": [
    "#EDA Visualization for Positive Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8cX1vkPRTRK"
   },
   "outputs": [],
   "source": [
    "stopwords_set = set(STOPWORDS)\n",
    "wordcloud = WordCloud(background_color='black',\n",
    "                     stopwords = stopwords_set,\n",
    "                      max_words = 300,\n",
    "                      max_font_size = 40,\n",
    "                      scale = 2,\n",
    "                      random_state=42\n",
    "                     ).generate(str(tweet_pos['_1']))\n",
    "\n",
    "print(wordcloud)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Azg_kVkXRX24"
   },
   "outputs": [],
   "source": [
    "tweets_df['_1'] = tweets_df['_1'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cR2XI_hXRZOC"
   },
   "outputs": [],
   "source": [
    "tweets_df['_2'] = tweets_df['_2'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyaj5xDbRaVG"
   },
   "outputs": [],
   "source": [
    "processed_txt = tweets_df['_1']\n",
    "sentiments = tweets_df['_2']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "o6x11WpGlOQL"
   },
   "source": [
    "#Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1guFGH0RdEr"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed_txt, sentiments,\n",
    "                                                    test_size = 0.3, random_state = 0)\n",
    "print(f'Data Split done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e5prGFzhlS3K"
   },
   "source": [
    "#Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vv4Oq5KCReUr"
   },
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print(f'Vectoriser fitted.')\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QmjnHRuNlWV_"
   },
   "source": [
    "#Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k33FRYB2Rfhx"
   },
   "outputs": [],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)\n",
    "print(f'Data Transformed.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cG7Ks4QLla2G"
   },
   "source": [
    "#Creating Data Model For Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OEHYgN0Xiqz"
   },
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(solver='liblinear', random_state=0)\n",
    "LRmodel.fit(X_train, y_train)\n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tC5jBxHeXr9W"
   },
   "outputs": [],
   "source": [
    "LRmodel = LogisticRegression(solver='liblinear', random_state=0).fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIBN2YvQX8Mm"
   },
   "outputs": [],
   "source": [
    "LRmodel.predict_proba(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9VJqTGa0liHV"
   },
   "source": [
    "#Data Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2R5PYVWYm2W"
   },
   "outputs": [],
   "source": [
    "print('Classification Report: \\n',classification_report(y_test, LRmodel.predict(X_test)))\n",
    "print('Confusion Matrix: \\n',metrics.confusion_matrix(y_test, LRmodel.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Tweets using API",
   "notebookOrigID": 700085199382562,
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
